{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dd4ff981-57b0-4d9d-8a15-8105ce57ab32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2024-25']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_134367/3912190453.py:450: FutureWarning: The default value of numeric_only in DataFrameGroupBy.sum is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  pd.DataFrame(g.sum()[f'{playtype}_PTS']).reset_index(drop=False),\n",
      "/tmp/ipykernel_134367/3912190453.py:450: FutureWarning: The default value of numeric_only in DataFrameGroupBy.sum is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  pd.DataFrame(g.sum()[f'{playtype}_PTS']).reset_index(drop=False),\n",
      "/tmp/ipykernel_134367/3912190453.py:450: FutureWarning: The default value of numeric_only in DataFrameGroupBy.sum is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  pd.DataFrame(g.sum()[f'{playtype}_PTS']).reset_index(drop=False),\n",
      "/tmp/ipykernel_134367/3912190453.py:450: FutureWarning: The default value of numeric_only in DataFrameGroupBy.sum is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  pd.DataFrame(g.sum()[f'{playtype}_PTS']).reset_index(drop=False),\n",
      "/tmp/ipykernel_134367/3912190453.py:450: FutureWarning: The default value of numeric_only in DataFrameGroupBy.sum is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  pd.DataFrame(g.sum()[f'{playtype}_PTS']).reset_index(drop=False),\n",
      "/tmp/ipykernel_134367/3912190453.py:450: FutureWarning: The default value of numeric_only in DataFrameGroupBy.sum is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  pd.DataFrame(g.sum()[f'{playtype}_PTS']).reset_index(drop=False),\n",
      "/tmp/ipykernel_134367/3912190453.py:450: FutureWarning: The default value of numeric_only in DataFrameGroupBy.sum is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  pd.DataFrame(g.sum()[f'{playtype}_PTS']).reset_index(drop=False),\n",
      "/tmp/ipykernel_134367/3912190453.py:450: FutureWarning: The default value of numeric_only in DataFrameGroupBy.sum is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  pd.DataFrame(g.sum()[f'{playtype}_PTS']).reset_index(drop=False),\n",
      "/tmp/ipykernel_134367/3912190453.py:450: FutureWarning: The default value of numeric_only in DataFrameGroupBy.sum is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  pd.DataFrame(g.sum()[f'{playtype}_PTS']).reset_index(drop=False),\n",
      "/tmp/ipykernel_134367/3912190453.py:450: FutureWarning: The default value of numeric_only in DataFrameGroupBy.sum is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  pd.DataFrame(g.sum()[f'{playtype}_PTS']).reset_index(drop=False),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Year 2024-25 is finished.\n",
      "Year Saved\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import unidecode\n",
    "import requests\n",
    "import json\n",
    "import time\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "import scipy.cluster.hierarchy as shc\n",
    "from kneed import KneeLocator\n",
    "\n",
    "headers = {'Host': 'stats.nba.com', 'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:72.0) Gecko/20100101 Firefox/72.0', 'Accept': 'application/json, text/plain, */*', 'Accept-Language': 'en-US,en;q=0.5', 'Accept-Encoding': 'gzip, deflate, br', 'x-nba-stats-origin': 'stats', 'x-nba-stats-token': 'true', 'Connection': 'keep-alive', 'Referer': 'https://stats.nba.com/', 'Pragma': 'no-cache', 'Cache-Control': 'no-cache'}\n",
    "\n",
    "df_list = []\n",
    "#,'2014-15','2015-16','2016-17','2017-18','2018-19','2019-20','2020-21','2021-22','2022-23'\n",
    "seasons=[str(i-1)+'-'+str(i)[-2:] for i in range(2025,2026)]\n",
    "print(seasons)\n",
    "for ssn in seasons:\n",
    "\n",
    "\n",
    "    # retrieve the 22 data sets that will be merged to include all of our final 189 features\n",
    "\n",
    "    #############################\n",
    "    # basic stats\n",
    "\n",
    "    url = 'https://stats.nba.com/stats/leaguedashplayerstats?College=&Conference=&Country=&DateFrom=&DateTo=&Division=&DraftPick=&DraftYear=&GameScope=&GameSegment=&Height=&LastNGames=0&LeagueID=00&Location=&MeasureType=Base&Month=0&OpponentTeamID=0&Outcome=&PORound=0&PaceAdjust=N&PerMode=Totals&Period=0&PlayerExperience=&PlayerPosition=&PlusMinus=N&Rank=N&Season=' + str(ssn) + '&SeasonSegment=&SeasonType=Regular+Season&ShotClockRange=&StarterBench=&TeamID=0&TwoWay=0&VsConference=&VsDivision=&Weight='\n",
    "\n",
    "    json = requests.get(url, headers=headers).json()\n",
    "    time.sleep(1)\n",
    "\n",
    "    data = json['resultSets'][0]['rowSet']\n",
    "    columns = json['resultSets'][0]['headers']\n",
    "\n",
    "    df = pd.DataFrame.from_records(data, columns=columns) \n",
    "    df.head()\n",
    "\n",
    "    #############################\n",
    "    # height / weight\n",
    "\n",
    "    url = 'https://stats.nba.com/stats/leaguedashplayerbiostats?College=&Conference=&Country=&DateFrom=&DateTo=&Division=&DraftPick=&DraftYear=&GameScope=&GameSegment=&Height=&LastNGames=0&LeagueID=00&Location=&Month=0&OpponentTeamID=0&Outcome=&PORound=0&PerMode=PerGame&Period=0&PlayerExperience=&PlayerPosition=&Season=' + str(ssn) + '&SeasonSegment=&SeasonType=Regular+Season&ShotClockRange=&StarterBench=&TeamID=0&VsConference=&VsDivision=&Weight='\n",
    "\n",
    "    json = requests.get(url, headers=headers).json()\n",
    "    time.sleep(1)\n",
    "\n",
    "    data = json['resultSets'][0]['rowSet']\n",
    "    columns = json['resultSets'][0]['headers']\n",
    "\n",
    "    db = pd.DataFrame.from_records(data, columns=columns) \n",
    "    db.head()\n",
    "\n",
    "    #############################\n",
    "    # deflections\n",
    "\n",
    "    url = 'https://stats.nba.com/stats/leaguehustlestatsplayer?College=&Conference=&Country=&DateFrom=&DateTo=&Division=&DraftPick=&DraftYear=&GameScope=&Height=&LastNGames=0&LeagueID=00&Location=&Month=0&OpponentTeamID=0&Outcome=&PORound=0&PaceAdjust=N&PerMode=Totals&PlayerExperience=&PlayerPosition=&PlusMinus=N&Rank=N&Season=' + str(ssn) + '&SeasonSegment=&SeasonType=Regular+Season&TeamID=0&VsConference=&VsDivision=&Weight='\n",
    "\n",
    "    json = requests.get(url, headers=headers).json()\n",
    "    time.sleep(1)\n",
    "\n",
    "    data = json['resultSets'][0]['rowSet']\n",
    "    columns = json['resultSets'][0]['headers']\n",
    "\n",
    "    df2 = pd.DataFrame.from_records(data, columns=columns) \n",
    "\n",
    "    #############################\n",
    "    # passing\n",
    "\n",
    "    url = 'https://stats.nba.com/stats/leaguedashptstats?College=&Conference=&Country=&DateFrom=&DateTo=&Division=&DraftPick=&DraftYear=&GameScope=&Height=&LastNGames=0&LeagueID=00&Location=&Month=0&OpponentTeamID=0&Outcome=&PORound=0&PerMode=Totals&PlayerExperience=&PlayerOrTeam=Player&PlayerPosition=&PtMeasureType=Passing&Season=' + str(ssn) + '&SeasonSegment=&SeasonType=Regular+Season&StarterBench=&TeamID=0&VsConference=&VsDivision=&Weight='\n",
    "\n",
    "    json = requests.get(url, headers=headers).json()\n",
    "    time.sleep(1)\n",
    "\n",
    "    data = json['resultSets'][0]['rowSet']\n",
    "    columns = json['resultSets'][0]['headers']\n",
    "\n",
    "    df3 = pd.DataFrame.from_records(data, columns=columns) \n",
    "\n",
    "    #############################\n",
    "    # speed / distance\n",
    "\n",
    "    url = 'https://stats.nba.com/stats/leaguedashptstats?College=&Conference=&Country=&DateFrom=&DateTo=&Division=&DraftPick=&DraftYear=&GameScope=&Height=&LastNGames=0&LeagueID=00&Location=&Month=0&OpponentTeamID=0&Outcome=&PORound=0&PerMode=Totals&PlayerExperience=&PlayerOrTeam=Player&PlayerPosition=&PtMeasureType=SpeedDistance&Season=' + str(ssn) + '&SeasonSegment=&SeasonType=Regular+Season&StarterBench=&TeamID=0&VsConference=&VsDivision=&Weight='\n",
    "\n",
    "    json = requests.get(url, headers=headers).json()\n",
    "    time.sleep(1)\n",
    "\n",
    "    data = json['resultSets'][0]['rowSet']\n",
    "    columns = json['resultSets'][0]['headers']\n",
    "\n",
    "    df4 = pd.DataFrame.from_records(data, columns=columns) \n",
    "\n",
    "    #############################\n",
    "    # touches\n",
    "\n",
    "    url = 'https://stats.nba.com/stats/leaguedashptstats?College=&Conference=&Country=&DateFrom=&DateTo=&Division=&DraftPick=&DraftYear=&GameScope=&Height=&LastNGames=0&LeagueID=00&Location=&Month=0&OpponentTeamID=0&Outcome=&PORound=0&PerMode=Totals&PlayerExperience=&PlayerOrTeam=Player&PlayerPosition=&PtMeasureType=Possessions&Season=' + str(ssn) + '&SeasonSegment=&SeasonType=Regular+Season&StarterBench=&TeamID=0&VsConference=&VsDivision=&Weight='\n",
    "\n",
    "    json = requests.get(url, headers=headers).json()\n",
    "    time.sleep(1)\n",
    "    data = json['resultSets'][0]['rowSet']\n",
    "    columns = json['resultSets'][0]['headers']\n",
    "\n",
    "    df5 = pd.DataFrame.from_records(data, columns=columns) \n",
    "\n",
    "    #############################\n",
    "    # scoring\n",
    "\n",
    "    url = 'https://stats.nba.com/stats/leaguedashptstats?College=&Conference=&Country=&DateFrom=&DateTo=&Division=&DraftPick=&DraftYear=&GameScope=&Height=&LastNGames=0&LeagueID=00&Location=&Month=0&OpponentTeamID=0&Outcome=&PORound=0&PerMode=Totals&PlayerExperience=&PlayerOrTeam=Player&PlayerPosition=&PtMeasureType=Efficiency&Season=' + str(ssn) + '&SeasonSegment=&SeasonType=Regular+Season&StarterBench=&TeamID=0&VsConference=&VsDivision=&Weight='\n",
    "\n",
    "    json = requests.get(url, headers=headers).json()\n",
    "    time.sleep(1)\n",
    "\n",
    "    data = json['resultSets'][0]['rowSet']\n",
    "    columns = json['resultSets'][0]['headers']\n",
    "\n",
    "    df6 = pd.DataFrame.from_records(data, columns=columns) \n",
    "\n",
    "    #############################\n",
    "    # defense\n",
    "\n",
    "    url = 'https://stats.nba.com/stats/leaguedashptdefend?College=&Conference=&Country=&DateFrom=&DateTo=&DefenseCategory=Overall&Division=&DraftPick=&DraftYear=&GameSegment=&Height=&LastNGames=0&LeagueID=00&Location=&Month=0&OpponentTeamID=0&Outcome=&PORound=0&PerMode=Totals&Period=0&PlayerExperience=&PlayerPosition=&Season=' + str(ssn) + '&SeasonSegment=&SeasonType=Regular+Season&StarterBench=&TeamID=0&VsConference=&VsDivision=&Weight='\n",
    "\n",
    "    json = requests.get(url, headers=headers).json()\n",
    "    time.sleep(1)\n",
    "\n",
    "    data = json['resultSets'][0]['rowSet']\n",
    "    columns = json['resultSets'][0]['headers']\n",
    "\n",
    "    df7 = pd.DataFrame.from_records(data, columns=columns) \n",
    "\n",
    "    #############################\n",
    "    # advanced\n",
    "\n",
    "    url = 'https://stats.nba.com/stats/leaguedashplayerstats?College=&Conference=&Country=&DateFrom=&DateTo=&Division=&DraftPick=&DraftYear=&GameScope=&GameSegment=&Height=&LastNGames=0&LeagueID=00&Location=&MeasureType=Advanced&Month=0&OpponentTeamID=0&Outcome=&PORound=0&PaceAdjust=N&PerMode=PerGame&Period=0&PlayerExperience=&PlayerPosition=&PlusMinus=N&Rank=N&Season=' + str(ssn) + '&SeasonSegment=&SeasonType=Regular+Season&ShotClockRange=&StarterBench=&TeamID=0&TwoWay=0&VsConference=&VsDivision=&Weight='\n",
    "\n",
    "    json = requests.get(url, headers=headers).json()\n",
    "    time.sleep(1)\n",
    "\n",
    "    data = json['resultSets'][0]['rowSet']\n",
    "    columns = json['resultSets'][0]['headers']\n",
    "\n",
    "    df8 = pd.DataFrame.from_records(data, columns=columns) \n",
    "\n",
    "    #############################\n",
    "    # misc scoring\n",
    "\n",
    "    url = 'https://stats.nba.com/stats/leaguedashplayerstats?College=&Conference=&Country=&DateFrom=&DateTo=&Division=&DraftPick=&DraftYear=&GameScope=&GameSegment=&Height=&LastNGames=0&LeagueID=00&Location=&MeasureType=Misc&Month=0&OpponentTeamID=0&Outcome=&PORound=0&PaceAdjust=N&PerMode=Totals&Period=0&PlayerExperience=&PlayerPosition=&PlusMinus=N&Rank=N&Season=' + str(ssn) + '&SeasonSegment=&SeasonType=Regular+Season&ShotClockRange=&StarterBench=&TeamID=0&TwoWay=0&VsConference=&VsDivision=&Weight='\n",
    "\n",
    "    json = requests.get(url, headers=headers).json()\n",
    "    time.sleep(1)\n",
    "\n",
    "    data = json['resultSets'][0]['rowSet']\n",
    "    columns = json['resultSets'][0]['headers']\n",
    "\n",
    "    df9 = pd.DataFrame.from_records(data, columns=columns) \n",
    "\n",
    "    #############################\n",
    "    # dws\n",
    "\n",
    "    url = 'https://stats.nba.com/stats/leaguedashplayerstats?College=&Conference=&Country=&DateFrom=&DateTo=&Division=&DraftPick=&DraftYear=&GameScope=&GameSegment=&Height=&LastNGames=0&LeagueID=00&Location=&MeasureType=Defense&Month=0&OpponentTeamID=0&Outcome=&PORound=0&PaceAdjust=N&PerMode=Totals&Period=0&PlayerExperience=&PlayerPosition=&PlusMinus=N&Rank=N&Season=' + str(ssn) + '&SeasonSegment=&SeasonType=Regular+Season&ShotClockRange=&StarterBench=&TeamID=0&TwoWay=0&VsConference=&VsDivision=&Weight='\n",
    "\n",
    "    json = requests.get(url, headers=headers).json()\n",
    "    time.sleep(1)\n",
    "\n",
    "    data = json['resultSets'][0]['rowSet']\n",
    "    columns = json['resultSets'][0]['headers']\n",
    "\n",
    "    df10 = pd.DataFrame.from_records(data, columns=columns) \n",
    "\n",
    "    #############################\n",
    "    # rebounding\n",
    "\n",
    "    url = 'https://stats.nba.com/stats/leaguedashptstats?College=&Conference=&Country=&DateFrom=&DateTo=&Division=&DraftPick=&DraftYear=&GameScope=&Height=&LastNGames=0&LeagueID=00&Location=&Month=0&OpponentTeamID=0&Outcome=&PORound=0&PerMode=Totals&PlayerExperience=&PlayerOrTeam=Player&PlayerPosition=&PtMeasureType=Rebounding&Season=' + str(ssn) + '&SeasonSegment=&SeasonType=Regular+Season&StarterBench=&TeamID=0&VsConference=&VsDivision=&Weight='\n",
    "\n",
    "    json = requests.get(url, headers=headers).json()\n",
    "    time.sleep(1)\n",
    "\n",
    "    data = json['resultSets'][0]['rowSet']\n",
    "    columns = json['resultSets'][0]['headers']\n",
    "\n",
    "    df11 = pd.DataFrame.from_records(data, columns=columns) # rebounding\n",
    "\n",
    "    #############################\n",
    "    # shooting zones\n",
    "\n",
    "    url = 'https://stats.nba.com/stats/leaguedashplayershotlocations?College=&Conference=&Country=&DateFrom=&DateTo=&DistanceRange=By%20Zone&Division=&DraftPick=&DraftYear=&GameScope=&GameSegment=&Height=&ISTRound=&LastNGames=0&Location=&MeasureType=Base&Month=0&OpponentTeamID=0&Outcome=&PORound=0&PaceAdjust=N&PerMode=PerGame&Period=0&PlayerExperience=&PlayerPosition=&PlusMinus=N&Rank=N&Season=2023-24&SeasonSegment=&SeasonType=Regular%20Season&ShotClockRange=&StarterBench=&TeamID=0&VsConference=&VsDivision=&Weight='\n",
    "\n",
    "    json = requests.get(url, headers=headers).json()\n",
    "    time.sleep(1)\n",
    "\n",
    "    data = json['resultSets']['rowSet']\n",
    "    columns = json['resultSets']['headers'][1]['columnNames']\n",
    "\n",
    "    df12 = pd.DataFrame.from_records(data, columns=columns) \n",
    "    #print(df12.columns)\n",
    "\n",
    "    df12.columns = [\n",
    "    'PLAYER_ID', 'PLAYER_NAME', 'TEAM_ID', 'TEAM_ABBREVIATION', 'AGE', 'NICKNAME',\n",
    "    'RA_FGM', 'RA_FGA', 'RA_FG_PCT', \n",
    "    'PAINT_FGM', 'PAINT_FGA', 'PAINT_FG_PCT', \n",
    "    'MIDRANGE_FGM', 'MIDRANGE_FGA', 'MIDRANGE_FG_PCT', \n",
    "    'LC3_FGM', 'LC3_FGA', 'LC3_FG_PCT', \n",
    "    'RC3_FGM', 'RC3_FGA', 'RC3_FG_PCT', \n",
    "    'C3_FGM', 'C3_FGA', 'C3_FG_PCT', \n",
    "    'AB3_FGM', 'AB3_FGA', 'AB3_FG_PCT', \n",
    "    'OTHER_FGM', 'OTHER_FGA', 'OTHER_FG_PCT'\n",
    "]\n",
    "\n",
    "    #############################\n",
    "    # assisted / unassisted scoring\n",
    "\n",
    "    url = 'https://stats.nba.com/stats/leaguedashplayerstats?College=&Conference=&Country=&DateFrom=&DateTo=&Division=&DraftPick=&DraftYear=&GameScope=&GameSegment=&Height=&LastNGames=0&LeagueID=00&Location=&MeasureType=Scoring&Month=0&OpponentTeamID=0&Outcome=&PORound=0&PaceAdjust=N&PerMode=PerGame&Period=0&PlayerExperience=&PlayerPosition=&PlusMinus=N&Rank=N&Season=' + str(ssn) + '&SeasonSegment=&SeasonType=Regular+Season&ShotClockRange=&StarterBench=&TeamID=0&TwoWay=0&VsConference=&VsDivision=&Weight='\n",
    "\n",
    "    json = requests.get(url, headers=headers).json()\n",
    "    time.sleep(1)\n",
    "\n",
    "    data = json['resultSets'][0]['rowSet']\n",
    "    columns = json['resultSets'][0]['headers']\n",
    "\n",
    "    df13 = pd.DataFrame.from_records(data, columns=columns) \n",
    "\n",
    "    #############################\n",
    "    # usage stats\n",
    "\n",
    "    url = 'https://stats.nba.com/stats/leaguedashplayerstats?College=&Conference=&Country=&DateFrom=&DateTo=&Division=&DraftPick=&DraftYear=&GameScope=&GameSegment=&Height=&LastNGames=0&LeagueID=00&Location=&MeasureType=Usage&Month=0&OpponentTeamID=0&Outcome=&PORound=0&PaceAdjust=N&PerMode=PerGame&Period=0&PlayerExperience=&PlayerPosition=&PlusMinus=N&Rank=N&Season=' + str(ssn) + '&SeasonSegment=&SeasonType=Regular+Season&ShotClockRange=&StarterBench=&TeamID=0&TwoWay=0&VsConference=&VsDivision=&Weight='\n",
    "\n",
    "    json = requests.get(url, headers=headers).json()\n",
    "    time.sleep(1)\n",
    "\n",
    "    data = json['resultSets'][0]['rowSet']\n",
    "    columns = json['resultSets'][0]['headers']\n",
    "\n",
    "    df14 = pd.DataFrame.from_records(data, columns=columns) \n",
    "\n",
    "    #############################\n",
    "    # isolation plays\n",
    "\n",
    "\n",
    "\n",
    "    # drives\n",
    "\n",
    "    url = 'https://stats.nba.com/stats/leaguedashptstats?College=&Conference=&Country=&DateFrom=&DateTo=&Division=&DraftPick=&DraftYear=&GameScope=&Height=&LastNGames=0&LeagueID=00&Location=&Month=0&OpponentTeamID=0&Outcome=&PORound=0&PerMode=Totals&PlayerExperience=&PlayerOrTeam=Player&PlayerPosition=&PtMeasureType=Drives&Season=' + str(ssn) + '&SeasonSegment=&SeasonType=Regular+Season&StarterBench=&TeamID=0&VsConference=&VsDivision=&Weight='\n",
    "\n",
    "    json = requests.get(url, headers=headers).json()\n",
    "    time.sleep(1)\n",
    "\n",
    "    data = json['resultSets'][0]['rowSet']\n",
    "    columns = json['resultSets'][0]['headers']\n",
    "\n",
    "    df19 = pd.DataFrame.from_records(data, columns=columns) \n",
    "\n",
    "    #############################\n",
    "    # advanced value stats\n",
    "\n",
    "    df20 = pd.read_html('https://www.basketball-reference.com/leagues/NBA_' + str(ssn[0:2] + ssn[-2:]) + '_advanced.html', header=0)[0][['Player','PER','OWS','DWS','WS','WS/48','OBPM','DBPM','BPM','VORP']]\n",
    "\n",
    "    df20.columns = ['PLAYER_NAME','PER','OWS','DWS','WS','WS/48','OBPM','DBPM','BPM','VORP']\n",
    "\n",
    "    df20 = df20[df20.PLAYER_NAME !='Player'].reset_index(drop=True)\n",
    "\n",
    "    df20 = df20.drop_duplicates(subset=['PLAYER_NAME'],keep='first').reset_index(drop=True)\n",
    "    df20.head()\n",
    "\n",
    "    # basketball-reference names sometime differ from stats.nba.com names, so these discrepancies must be fixed\n",
    "\n",
    "    def remove_accents(a):\n",
    "        return unidecode.unidecode(a).strip('*')\n",
    "    df20['PLAYER_NAME'] = df20['PLAYER_NAME'].apply(remove_accents)\n",
    "\n",
    "    df20['PLAYER_NAME'].replace({'Robert Williams':'Robert Williams III','Marcus Morris':'Marcus Morris Sr.','Derrick Walton':'Derrick Walton Jr.','Juan Hernangomez':'Juancho Hernangomez','Sviatoslav Mykhailiuk':'Svi Mykhailiuk','Zach Norvell':'Zach Norvell Jr.','Lonnie Walker':'Lonnie Walker IV','Charlie Brown':'Charles Brown Jr.','C.J. Miles':'CJ Miles','Wesley Iwundu':'Wes Iwundu','J.J. Redick':'JJ Redick','B.J. Johnson':'BJ Johnson','Melvin Frazier':'Melvin Frazier Jr.','Otto Porter':'Otto Porter Jr.','James Ennis':'James Ennis III','Danuel House':'Danuel House Jr.','Brian Bowen':'Brian Bowen II','Kevin Knox':'Kevin Knox II','Frank Mason III':'Frank Mason','Harry Giles':'Harry Giles III','T.J. Leaf':'TJ Leaf','J.R. Smith':'JR Smith','Vince Edwards':'Vincent Edwards','D.J. Stephens':'DJ Stephens','Mitch Creek':'Mitchell Creek','R.J. Hunter':'RJ Hunter','Wade Baldwin':'Wade Baldwin IV','J.J. Hickson':'JJ Hickson','D.J. White':'DJ White','Glen Rice Jr.':'Glen Rice','A.J. Price':'AJ Price','Jeff Taylor':'Jeffery Taylor','Perry Jones':'Perry Jones III','Vitor Luiz Faverani':'Vitor Faverani','Hamady N\\'Diaye':'Hamady Ndiaye','Roger Mason':'Roger Mason Jr.','K.J. McDaniels':'KJ McDaniels','P.J. Hairston':'PJ Hairston','Johnny O\\'Bryant':'Johnny O\\'Bryant III','J.J. O\\'Brien':'JJ O\\'Brien','A.J. Hammons':'AJ Hammons','Vince Hunter':'Vincent Hunter','Andrew White':'Andrew White III','Matt Williams':'Matt Williams Jr.'},inplace=True)\n",
    "\n",
    "    #############################\n",
    "    # positions\n",
    "\n",
    "    url = 'https://stats.nba.com/stats/leaguedashplayerstats?College=&Conference=&Country=&DateFrom=&DateTo=&Division=&DraftPick=&DraftYear=&GameScope=&GameSegment=&Height=&LastNGames=0&LeagueID=00&Location=&MeasureType=Base&Month=0&OpponentTeamID=0&Outcome=&PORound=0&PaceAdjust=N&PerMode=Totals&Period=0&PlayerExperience=&PlayerPosition=C&PlusMinus=N&Rank=N&Season=' +str(ssn) + '&SeasonSegment=&SeasonType=Regular+Season&ShotClockRange=&StarterBench=&TeamID=0&TwoWay=0&VsConference=&VsDivision=&Weight='\n",
    "\n",
    "    json = requests.get(url, headers=headers).json()\n",
    "    time.sleep(1)\n",
    "\n",
    "    data = json['resultSets'][0]['rowSet']\n",
    "    columns = json['resultSets'][0]['headers']\n",
    "\n",
    "    c_df = pd.DataFrame.from_records(data, columns=columns) # CENTERS\n",
    "\n",
    "    ###\n",
    "\n",
    "    url = 'https://stats.nba.com/stats/leaguedashplayerstats?College=&Conference=&Country=&DateFrom=&DateTo=&Division=&DraftPick=&DraftYear=&GameScope=&GameSegment=&Height=&LastNGames=0&LeagueID=00&Location=&MeasureType=Base&Month=0&OpponentTeamID=0&Outcome=&PORound=0&PaceAdjust=N&PerMode=Totals&Period=0&PlayerExperience=&PlayerPosition=G&PlusMinus=N&Rank=N&Season=' + str(ssn) + '&SeasonSegment=&SeasonType=Regular+Season&ShotClockRange=&StarterBench=&TeamID=0&TwoWay=0&VsConference=&VsDivision=&Weight='\n",
    "\n",
    "    json = requests.get(url, headers=headers).json()\n",
    "    time.sleep(1)\n",
    "\n",
    "    data = json['resultSets'][0]['rowSet']\n",
    "    columns = json['resultSets'][0]['headers']\n",
    "\n",
    "    g_df = pd.DataFrame.from_records(data, columns=columns) # GUARDS\n",
    "\n",
    "    ###\n",
    "\n",
    "    url = 'https://stats.nba.com/stats/leaguedashplayerstats?College=&Conference=&Country=&DateFrom=&DateTo=&Division=&DraftPick=&DraftYear=&GameScope=&GameSegment=&Height=&LastNGames=0&LeagueID=00&Location=&MeasureType=Base&Month=0&OpponentTeamID=0&Outcome=&PORound=0&PaceAdjust=N&PerMode=Totals&Period=0&PlayerExperience=&PlayerPosition=F&PlusMinus=N&Rank=N&Season=' + str(ssn) + '&SeasonSegment=&SeasonType=Regular+Season&ShotClockRange=&StarterBench=&TeamID=0&TwoWay=0&VsConference=&VsDivision=&Weight='\n",
    "\n",
    "    json = requests.get(url, headers=headers).json()\n",
    "    time.sleep(1)\n",
    "\n",
    "    data = json['resultSets'][0]['rowSet']\n",
    "    columns = json['resultSets'][0]['headers']\n",
    "\n",
    "    f_df = pd.DataFrame.from_records(data, columns=columns) # FORWARDS\n",
    "\n",
    "    c_df['position'] = 'C'\n",
    "    g_df['position'] = 'G'\n",
    "    f_df['position'] = 'F'\n",
    "\n",
    "    pf = pd.concat([c_df,g_df,f_df]).reset_index(drop=True)\n",
    "    pf = pf[['PLAYER_NAME','PLAYER_ID','AGE','MIN','position']]\n",
    "    pf.columns = ['player','player_id','age','mp','position']\n",
    "\n",
    "    row_list = []\n",
    "\n",
    "    for n in pf.player.unique():\n",
    "\n",
    "        tf = pf[pf.player == n].reset_index(drop=True)\n",
    "        posf = pd.DataFrame(tf.groupby('position')['mp'].sum()).sort_values(by='mp',ascending=False).reset_index(drop=False)\n",
    "\n",
    "        pos1 = posf.position[0]\n",
    "\n",
    "        if posf.shape[0] > 1:\n",
    "            pos2 = posf.position[1]\n",
    "        else:\n",
    "            pos2 = None\n",
    "        if posf.shape[0] > 2:\n",
    "            pos3 = posf.position[2]\n",
    "        else:\n",
    "            pos3 = None\n",
    "\n",
    "        dict1 = {'player':n,'pos1':pos1,'pos2':pos2,'pos3':pos3}\n",
    "\n",
    "        row_list.append(dict1)\n",
    "\n",
    "    pfin = pd.DataFrame(row_list)[['player','pos1','pos2','pos3']]\n",
    "    pfin.columns = ['PLAYER_NAME','POSITION','pos2','pos3']\n",
    "\n",
    "    #############################\n",
    "    # now, all 22 dataframes can be merged into one: df\n",
    "\n",
    "    df = df[['PLAYER_ID','PLAYER_NAME','AGE','GP','MIN','FGM','FGA','FG3M','FG3A','FTM','FTA','OREB','DREB','REB','AST','TOV','STL','BLK','BLKA','PF','PFD','PTS']]\n",
    "\n",
    "    df = pd.merge(df,pfin[['PLAYER_NAME','POSITION']],on='PLAYER_NAME')\n",
    "\n",
    "    df = pd.merge(df,pd.get_dummies(df.POSITION), left_index=True, right_index=True) # one-hot encoding on position to create three columns: C, G, F\n",
    "\n",
    "    df = pd.merge(df, db[['PLAYER_NAME', 'NET_RATING', 'OREB_PCT', 'DREB_PCT', 'USG_PCT', 'TS_PCT', 'AST_PCT']], on='PLAYER_NAME', how='left')\n",
    "\n",
    "    df = pd.merge(df,df2[['PLAYER_NAME','CONTESTED_SHOTS','CONTESTED_SHOTS_2PT','CONTESTED_SHOTS_3PT','DEFLECTIONS','CHARGES_DRAWN','SCREEN_ASSISTS','SCREEN_AST_PTS','OFF_LOOSE_BALLS_RECOVERED','DEF_LOOSE_BALLS_RECOVERED','LOOSE_BALLS_RECOVERED','OFF_BOXOUTS','DEF_BOXOUTS','BOX_OUT_PLAYER_TEAM_REBS','BOX_OUT_PLAYER_REBS','BOX_OUTS']],on='PLAYER_NAME',how='left')\n",
    "\n",
    "    df = pd.merge(df,df3[['PLAYER_NAME','PASSES_MADE','PASSES_RECEIVED','FT_AST','SECONDARY_AST','POTENTIAL_AST','AST_PTS_CREATED']],on='PLAYER_NAME',how='left')\n",
    "\n",
    "    df = pd.merge(df,df4[['PLAYER_NAME','DIST_MILES','DIST_MILES_OFF','DIST_MILES_DEF','AVG_SPEED','AVG_SPEED_OFF','AVG_SPEED_DEF']],on='PLAYER_NAME',how='left')\n",
    "\n",
    "    df = pd.merge(df,df5[['PLAYER_NAME','TOUCHES','FRONT_CT_TOUCHES','TIME_OF_POSS','AVG_SEC_PER_TOUCH','AVG_DRIB_PER_TOUCH','PTS_PER_TOUCH','ELBOW_TOUCHES','POST_TOUCHES']],on='PLAYER_NAME',how='left')\n",
    "\n",
    "    df = pd.merge(df,df6[['PLAYER_NAME','DRIVE_PTS','DRIVE_FG_PCT','CATCH_SHOOT_PTS','CATCH_SHOOT_FG_PCT','PULL_UP_PTS','PULL_UP_FG_PCT','PAINT_TOUCH_PTS','PAINT_TOUCH_FG_PCT','POST_TOUCH_PTS','POST_TOUCH_FG_PCT','ELBOW_TOUCH_PTS','ELBOW_TOUCH_FG_PCT']],on='PLAYER_NAME',how='left')\n",
    "\n",
    "    df = pd.merge(df,df7[['PLAYER_NAME','D_FGM','D_FGA','D_FG_PCT']],on='PLAYER_NAME',how='left')\n",
    "\n",
    "    df = pd.merge(df,df8[['PLAYER_NAME','OFF_RATING','DEF_RATING','AST_RATIO','PACE','PIE']],on='PLAYER_NAME',how='left')\n",
    "\n",
    "    df = pd.merge(df,df9[['PLAYER_NAME','PTS_OFF_TOV','PTS_2ND_CHANCE','PTS_FB','PTS_PAINT']],on='PLAYER_NAME',how='left')\n",
    "\n",
    "    df = pd.merge(df,df10[['PLAYER_NAME','DEF_WS']],on='PLAYER_NAME',how='left')\n",
    "\n",
    "    df = pd.merge(df,df11[['PLAYER_NAME','AVG_REB_DIST','REB_CHANCE_PCT','REB_CHANCE_PCT_ADJ','REB_CHANCES','REB_CONTEST_PCT']],on='PLAYER_NAME',how='left')\n",
    "\n",
    "    df = pd.merge(df,df12[['PLAYER_NAME','RA_FGM', 'RA_FGA', 'RA_FG_PCT', \n",
    "    'PAINT_FGM', 'PAINT_FGA', 'PAINT_FG_PCT', \n",
    "    'MIDRANGE_FGM', 'MIDRANGE_FGA', 'MIDRANGE_FG_PCT', \n",
    "    'LC3_FGM', 'LC3_FGA', 'LC3_FG_PCT', \n",
    "    'RC3_FGM', 'RC3_FGA', 'RC3_FG_PCT', \n",
    "    'C3_FGM', 'C3_FGA', 'C3_FG_PCT', \n",
    "    'AB3_FGM', 'AB3_FGA', 'AB3_FG_PCT']],on='PLAYER_NAME',how='left')\n",
    "\n",
    "    df = pd.merge(df,df13[['PLAYER_NAME','PCT_FGA_2PT','PCT_FGA_3PT','PCT_PTS_2PT','PCT_PTS_2PT_MR','PCT_PTS_3PT','PCT_PTS_FB','PCT_PTS_FT','PCT_PTS_OFF_TOV','PCT_PTS_PAINT','PCT_AST_2PM','PCT_UAST_2PM','PCT_AST_3PM','PCT_UAST_3PM', 'PCT_AST_FGM', 'PCT_UAST_FGM']],on='PLAYER_NAME',how='left')\n",
    "\n",
    "    df = pd.merge(df,df14[['PLAYER_NAME','PCT_FGM','PCT_FGA','PCT_FG3M','PCT_FG3A','PCT_FTM','PCT_FTA','PCT_OREB','PCT_DREB','PCT_REB','PCT_AST','PCT_TOV','PCT_STL','PCT_BLK','PCT_BLKA','PCT_PF','PCT_PFD','PCT_PTS']],on='PLAYER_NAME',how='left')\n",
    "\n",
    "    #df = pd.merge(df,df15[['PLAYER_NAME','ISO_POSS_PCT','ISO_EFG_PCT','ISO_PTS']],on='PLAYER_NAME',how='left')\n",
    "\n",
    "    #df = pd.merge(df,df16[['PLAYER_NAME','PRBH_POSS_PCT','PRBH_EFG_PCT','PRBH_PTS']],on='PLAYER_NAME',how='left')\n",
    "\n",
    "    #df = pd.merge(df,df17[['PLAYER_NAME','PRRM_POSS_PCT','PRRM_EFG_PCT','PRRM_PTS']],on='PLAYER_NAME',how='left')\n",
    "\n",
    "    #df = pd.merge(df,df18[['PLAYER_NAME','SU_POSS_PCT','SU_EFG_PCT','SU_PTS']],on='PLAYER_NAME',how='left')\n",
    "\n",
    "    df = pd.merge(df,df19[['PLAYER_NAME','DRIVES','DRIVE_AST_PCT','DRIVE_PASSES_PCT']],on='PLAYER_NAME',how='left')\n",
    "\n",
    "    df = pd.merge(df,df20[['PLAYER_NAME','PER','OWS','WS','WS/48','OBPM','DBPM','BPM','VORP']],on='PLAYER_NAME',how='left')\n",
    "\n",
    "    #############################\n",
    "    # feature engineering\n",
    "\n",
    "    df['TIME_OF_POSS_36'] = 36*(df['TIME_OF_POSS']/df.MIN)\n",
    "    df['OFB_PCT'] = 1-(df['TIME_OF_POSS']/df.MIN)\n",
    "    #df['PCT_PTS_ISO'] = df.ISO_PTS / df.PTS\n",
    "    #df['PCT_PTS_PRBH'] = df.PRBH_PTS / df.PTS\n",
    "    #df['PCT_PTS_PRRM'] = df.PRRM_PTS / df.PTS\n",
    "    #df['PCT_PTS_SU'] = df.SU_PTS / df.PTS\n",
    "    df['PCT_PTS_DRIVES'] = df.DRIVE_PTS / df.PTS\n",
    "\n",
    "    df['FG2M'] = df.FGM - df.FG3M\n",
    "    df['FG2A'] = df.FGA - df.FG3A\n",
    "\n",
    "    df['FG_PCT'] = df.FGM/df.FGA\n",
    "    df['FG2_PCT'] = df.FG2M/df.FG2A\n",
    "    df['FG3_PCT'] = df.FG3M/df.FG3A\n",
    "    df['FT_PCT'] = df.FTM/df.FTA\n",
    "\n",
    "    df['AST_TO'] = df.AST/df.TOV\n",
    "    df['MPG'] = df.MIN/df.GP\n",
    "    df['EFG'] = (df.FG2M + 1.5*df.FG3M) / df.FGA\n",
    "    df['FTR'] = df.FTA/df.FGA\n",
    "    playtypes = [\n",
    "        'Isolation', 'PRBallHandler', 'PRRollMan', 'Spotup', 'Postup', \n",
    "        'Transition', 'Handoff', 'Cut', 'OffScreen', 'Misc'\n",
    "    ]\n",
    "    \n",
    "    for playtype in playtypes:\n",
    "        url = 'https://stats.nba.com/stats/synergyplaytypes?LeagueID=00&PerMode=Totals&PlayType=' + playtype + '&PlayerOrTeam=P&SeasonType=Regular+Season&SeasonYear=' + str(ssn) + '&TypeGrouping=offensive'\n",
    "    \n",
    "        json = requests.get(url, headers=headers).json()\n",
    "        time.sleep(1)\n",
    "    \n",
    "        data = json['resultSets'][0]['rowSet']\n",
    "        columns = json['resultSets'][0]['headers']\n",
    "    \n",
    "        df_playtypes = pd.DataFrame.from_records(data, columns=columns)\n",
    "        \n",
    "        # Dynamically rename columns\n",
    "        df_playtypes.rename(columns={\n",
    "            'POSS_PCT': f'{playtype}_POSS_PCT',\n",
    "            'EFG_PCT': f'{playtype}_EFG_PCT',\n",
    "            'PTS': f'{playtype}_PTS'\n",
    "        }, inplace=True)\n",
    "    \n",
    "        g = df_playtypes.groupby(['PLAYER_NAME'])\n",
    "    \n",
    "        processed_df = pd.merge(\n",
    "            g.apply(lambda x: pd.Series(\n",
    "                np.average(x[[f'{playtype}_POSS_PCT',f'{playtype}_EFG_PCT']],\n",
    "                           weights=x['POSS'], axis=0),\n",
    "                [f'{playtype}_POSS_PCT',f'{playtype}_EFG_PCT']\n",
    "            )).reset_index(drop=False),\n",
    "            pd.DataFrame(g.sum()[f'{playtype}_PTS']).reset_index(drop=False),\n",
    "            on='PLAYER_NAME'\n",
    "        )\n",
    "    \n",
    "        # Merge processed play type data into main dataframe\n",
    "        df = pd.merge(df, processed_df, on='PLAYER_NAME', how='left')\n",
    "        \n",
    "    \n",
    "    # Add feature engineering for new play types\n",
    "    playtype_pts=[]\n",
    "    for playtype in playtypes:\n",
    "        playtype_pts.append(f'{playtype}_PTS')\n",
    "        df[f'PCT_PTS_{playtype.upper()}'] = df[f'{playtype}_PTS'] / df.PTS\n",
    "\n",
    "    # convert volume stats to per-36 stats\n",
    "    nlist= ['FGM','FGA','FG2M','FG2A','FG3M','FG3A','FTM','FTA','OREB','DREB','REB','AST','TOV','STL','BLK','BLKA','PF','PFD','PTS','CONTESTED_SHOTS','CONTESTED_SHOTS_2PT','CONTESTED_SHOTS_3PT','DEFLECTIONS','CHARGES_DRAWN','SCREEN_ASSISTS','SCREEN_AST_PTS','OFF_LOOSE_BALLS_RECOVERED','DEF_LOOSE_BALLS_RECOVERED','LOOSE_BALLS_RECOVERED','OFF_BOXOUTS','DEF_BOXOUTS','BOX_OUT_PLAYER_TEAM_REBS','BOX_OUT_PLAYER_REBS','BOX_OUTS','PASSES_MADE','PASSES_RECEIVED','FT_AST','SECONDARY_AST','POTENTIAL_AST','AST_PTS_CREATED','DIST_MILES','DIST_MILES_OFF','DIST_MILES_DEF','TOUCHES','FRONT_CT_TOUCHES','ELBOW_TOUCHES','POST_TOUCHES','DRIVE_PTS','CATCH_SHOOT_PTS','PULL_UP_PTS','PAINT_TOUCH_PTS','POST_TOUCH_PTS','ELBOW_TOUCH_PTS','D_FGM','D_FGA','PTS_OFF_TOV','PTS_2ND_CHANCE','PTS_FB','PTS_PAINT','REB_CHANCES','RA_FGM','RA_FGA','PAINT_FGM','PAINT_FGA','MIDRANGE_FGM','MIDRANGE_FGA','LC3_FGM','LC3_FGA','RC3_FGM','RC3_FGA','C3_FGM','C3_FGA','AB3_FGM','AB3_FGA','DRIVES','DRIVE_PTS']\n",
    "    nlist= nlist+playtype_pts\n",
    "    for n in nlist:\n",
    "    \n",
    "    #for n in ['FGM','FGA','FG2M','FG2A','FG3M','FG3A','FTM','FTA','OREB','DREB','REB','AST','TOV','STL','BLK','BLKA','PF','PFD','PTS','CONTESTED_SHOTS','CONTESTED_SHOTS_2PT','CONTESTED_SHOTS_3PT','DEFLECTIONS','CHARGES_DRAWN','SCREEN_ASSISTS','SCREEN_AST_PTS','OFF_LOOSE_BALLS_RECOVERED','DEF_LOOSE_BALLS_RECOVERED','LOOSE_BALLS_RECOVERED','OFF_BOXOUTS','DEF_BOXOUTS','BOX_OUT_PLAYER_TEAM_REBS','BOX_OUT_PLAYER_REBS','BOX_OUTS','PASSES_MADE','PASSES_RECEIVED','FT_AST','SECONDARY_AST','POTENTIAL_AST','AST_PTS_CREATED','DIST_MILES','DIST_MILES_OFF','DIST_MILES_DEF','TOUCHES','FRONT_CT_TOUCHES','ELBOW_TOUCHES','POST_TOUCHES','DRIVE_PTS','CATCH_SHOOT_PTS','PULL_UP_PTS','PAINT_TOUCH_PTS','POST_TOUCH_PTS','ELBOW_TOUCH_PTS','D_FGM','D_FGA','PTS_OFF_TOV','PTS_2ND_CHANCE','PTS_FB','PTS_PAINT','REB_CHANCES','ISO_PTS','PRBH_PTS','PRRM_PTS','SU_PTS','DRIVES','DRIVE_PTS']:\n",
    "\n",
    "        df[n] = 36*(df[n]/df.MIN)\n",
    "\n",
    "    # final cleaning up\n",
    "\n",
    "    #df['PLAYER_WEIGHT'] = pd.to_numeric(df['PLAYER_WEIGHT'])\n",
    "    df['PER'] = pd.to_numeric(df['PER'])\n",
    "    df['OWS'] = pd.to_numeric(df['OWS'])\n",
    "    df['WS'] = pd.to_numeric(df['WS'])\n",
    "    df['WS/48'] = pd.to_numeric(df['WS/48'])\n",
    "    df['OBPM'] = pd.to_numeric(df['OBPM'])\n",
    "    df['BPM'] = pd.to_numeric(df['DBPM'])\n",
    "    df['DBPM'] = pd.to_numeric(df['BPM'])\n",
    "    df['VORP'] = pd.to_numeric(df['VORP'])\n",
    "    df['AST_RATIO'] = df.apply(lambda x: x['AST'] if x['AST_RATIO'] == np.inf else x['AST_RATIO'], axis=1)\n",
    "    df['OFF_DIST_PER_SEC_POSS'] = np.where(df['TIME_OF_POSS'] != 0, df['DIST_MILES_OFF'] / df['TIME_OF_POSS'], 0)\n",
    "    df['TS%'] = df['PTS'] / (2 * (df['FGA'] + 0.44 * df['FTA']))\n",
    "    df['eFG%'] = (df['FGM'] + 0.5 * df['FG3M']) / df['FGA']\n",
    "    df['PASS_EFF'] = df['AST'] / df['PASSES_MADE']\n",
    "    \n",
    "    df['SEASON'] = ssn\n",
    "    df['year']=int(ssn.split('-')[0])+1\n",
    "    \n",
    "    df = df.drop_duplicates(subset=['PLAYER_NAME','AGE'])\n",
    "\n",
    "    df.fillna(0, inplace=True)\n",
    "\n",
    "    df_list.append(df)\n",
    "    print(f\"Year {ssn} is finished.\")\n",
    "    \n",
    "df = pd.concat(df_list).reset_index(drop=True)\n",
    "for year in range (df['year'].min(),df['year'].max()+1):\n",
    "    yeardf = df[df.year==year]\n",
    "    yeardf.to_csv('cluster_'+str(year)+'.csv',index=False)\n",
    "    print('Year Saved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87121458-1458-49a0-a942-9b95d9cffee7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-25\n",
      "     PLAYER_ID      PLAYER_NAME   AGE  GP          MIN        FGM        FGA  \\\n",
      "0      1631260         AJ Green  25.0  49  1105.520000   4.135610   9.736595   \n",
      "1       203932     Aaron Gordon  29.0  34   916.336667   5.971604  11.668201   \n",
      "2      1628988    Aaron Holiday  28.0  41   514.741667   4.825722  11.609707   \n",
      "3      1630174    Aaron Nesmith  25.0  20   418.781667   4.985892  10.315638   \n",
      "4      1630598    Aaron Wiggins  26.0  55  1212.246667   6.978778  14.046646   \n",
      "..         ...              ...   ...  ..          ...        ...        ...   \n",
      "382    1628380     Zach Collins  27.0  39   469.823333   5.057220  11.110559   \n",
      "383    1641744        Zach Edey  22.0  42   858.865000   6.664610  11.568756   \n",
      "384     203897      Zach LaVine  29.0  49  1688.888333   8.782108  17.564216   \n",
      "385    1630533  Ziaire Williams  23.0  42  1004.883333   4.728907  11.893918   \n",
      "386    1629627  Zion Williamson  24.0  20   562.526667  11.967433  21.694971   \n",
      "\n",
      "         FG3M      FG3A       FTM  ...  PCT_PTS_HANDOFF  PCT_PTS_CUT  \\\n",
      "0    3.451769  8.271221  0.521022  ...         0.159574     0.000000   \n",
      "1    1.610762  3.771540  3.339384  ...         0.000000     0.183721   \n",
      "2    2.587706  7.343489  1.748450  ...         0.000000     0.000000   \n",
      "3    1.891200  4.899928  2.664873  ...         0.000000     0.000000   \n",
      "4    2.524239  6.592718  1.336362  ...         0.035000     0.105000   \n",
      "..        ...       ...       ...  ...              ...          ...   \n",
      "382  1.149368  4.061101  2.681859  ...         0.000000     0.230769   \n",
      "383  0.628737  1.802379  2.179621  ...         0.000000     0.238961   \n",
      "384  3.197369  7.631055  3.730264  ...         0.040905     0.040905   \n",
      "385  1.970378  6.197734  2.722704  ...         0.063291     0.088608   \n",
      "386  0.191991  0.767964  7.231657  ...         0.051020     0.071429   \n",
      "\n",
      "     PCT_PTS_OFFSCREEN  PCT_PTS_MISC  OFF_DIST_PER_SEC_POSS       TS%  \\\n",
      "0             0.069149      0.029255               0.036674  0.612537   \n",
      "1             0.000000      0.013953               0.020021  0.624855   \n",
      "2             0.050000      0.045000               0.029153  0.560790   \n",
      "3             0.000000      0.100592               0.087038  0.626111   \n",
      "4             0.035000      0.006667               0.021867  0.604985   \n",
      "..                 ...           ...                    ...       ...   \n",
      "382           0.000000      0.043956               0.068597  0.561174   \n",
      "383           0.000000      0.036364               0.058479  0.622092   \n",
      "384           0.021758      0.010444               0.007067  0.625708   \n",
      "385           0.000000      0.032911               0.031136  0.530230   \n",
      "386           0.000000      0.010204               0.019190  0.588942   \n",
      "\n",
      "         eFG%  PASS_EFF   SEASON  year  \n",
      "0    0.602007  0.062196  2024-25  2025  \n",
      "1    0.580808  0.121381  2024-25  2025  \n",
      "2    0.527108  0.098004  2024-25  2025  \n",
      "3    0.575000  0.061644  2024-25  2025  \n",
      "4    0.586681  0.081596  2024-25  2025  \n",
      "..        ...       ...      ...   ...  \n",
      "382  0.506897  0.098418  2024-25  2025  \n",
      "383  0.603261  0.054834  2024-25  2025  \n",
      "384  0.591019  0.127221  2024-25  2025  \n",
      "385  0.480422  0.065817  2024-25  2025  \n",
      "386  0.556047  0.127660  2024-25  2025  \n",
      "\n",
      "[387 rows x 220 columns]\n",
      "CSV file 'nba_similar_players.csv' has been created with each player and their 5 most similar players.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input X contains infinity or a value too large for dtype('float64').",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 223\u001b[0m\n\u001b[1;32m    214\u001b[0m     testdf\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnba_analysis_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00myear\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.csv\u001b[39m\u001b[38;5;124m'\u001b[39m,index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    222\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m year \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m2025\u001b[39m,\u001b[38;5;241m2026\u001b[39m):\n\u001b[0;32m--> 223\u001b[0m     \u001b[43msave_cluster_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43myear\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[3], line 141\u001b[0m, in \u001b[0;36msave_cluster_data\u001b[0;34m(year)\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;66;03m# Standardize the data\u001b[39;00m\n\u001b[1;32m    140\u001b[0m x \u001b[38;5;241m=\u001b[39m testdf\u001b[38;5;241m.\u001b[39mloc[:, features]\u001b[38;5;241m.\u001b[39mvalues\n\u001b[0;32m--> 141\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[43mStandardScaler\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;66;03m# Perform PCA to reduce dimensionality\u001b[39;00m\n\u001b[1;32m    144\u001b[0m pca \u001b[38;5;241m=\u001b[39m PCA(n_components\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)  \u001b[38;5;66;03m# Reduce to 2 components for visualization\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/utils/_set_output.py:140\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 140\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    141\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m    142\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[1;32m    143\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[1;32m    144\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[1;32m    145\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[1;32m    146\u001b[0m         )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/base.py:878\u001b[0m, in \u001b[0;36mTransformerMixin.fit_transform\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    874\u001b[0m \u001b[38;5;66;03m# non-optimized default implementation; override when a better\u001b[39;00m\n\u001b[1;32m    875\u001b[0m \u001b[38;5;66;03m# method is possible for a given clustering algorithm\u001b[39;00m\n\u001b[1;32m    876\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    877\u001b[0m     \u001b[38;5;66;03m# fit method of arity 1 (unsupervised transformation)\u001b[39;00m\n\u001b[0;32m--> 878\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_params\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mtransform(X)\n\u001b[1;32m    879\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    880\u001b[0m     \u001b[38;5;66;03m# fit method of arity 2 (supervised transformation)\u001b[39;00m\n\u001b[1;32m    881\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\u001b[38;5;241m.\u001b[39mtransform(X)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:824\u001b[0m, in \u001b[0;36mStandardScaler.fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    822\u001b[0m \u001b[38;5;66;03m# Reset internal state before fitting\u001b[39;00m\n\u001b[1;32m    823\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()\n\u001b[0;32m--> 824\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpartial_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:861\u001b[0m, in \u001b[0;36mStandardScaler.partial_fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    858\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m    860\u001b[0m first_call \u001b[38;5;241m=\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn_samples_seen_\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 861\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    862\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    863\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsc\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    864\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mFLOAT_DTYPES\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    865\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_all_finite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mallow-nan\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    866\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfirst_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    867\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    868\u001b[0m n_features \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    870\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sample_weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/base.py:565\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[0;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[1;32m    563\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mValidation should be done on X, y or both.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    564\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m no_val_y:\n\u001b[0;32m--> 565\u001b[0m     X \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mX\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcheck_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    566\u001b[0m     out \u001b[38;5;241m=\u001b[39m X\n\u001b[1;32m    567\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_y:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/utils/validation.py:921\u001b[0m, in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m    915\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    916\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound array with dim \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m expected <= 2.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    917\u001b[0m             \u001b[38;5;241m%\u001b[39m (array\u001b[38;5;241m.\u001b[39mndim, estimator_name)\n\u001b[1;32m    918\u001b[0m         )\n\u001b[1;32m    920\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m force_all_finite:\n\u001b[0;32m--> 921\u001b[0m         \u001b[43m_assert_all_finite\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    922\u001b[0m \u001b[43m            \u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    923\u001b[0m \u001b[43m            \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    924\u001b[0m \u001b[43m            \u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    925\u001b[0m \u001b[43m            \u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_all_finite\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mallow-nan\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    926\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    928\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ensure_min_samples \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    929\u001b[0m     n_samples \u001b[38;5;241m=\u001b[39m _num_samples(array)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/utils/validation.py:161\u001b[0m, in \u001b[0;36m_assert_all_finite\u001b[0;34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m estimator_name \u001b[38;5;129;01mand\u001b[39;00m input_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m has_nan_error:\n\u001b[1;32m    145\u001b[0m     \u001b[38;5;66;03m# Improve the error message on how to handle missing values in\u001b[39;00m\n\u001b[1;32m    146\u001b[0m     \u001b[38;5;66;03m# scikit-learn.\u001b[39;00m\n\u001b[1;32m    147\u001b[0m     msg_err \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    148\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not accept missing values\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    149\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m encoded as NaN natively. For supervised learning, you might want\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    159\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m#estimators-that-handle-nan-values\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    160\u001b[0m     )\n\u001b[0;32m--> 161\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg_err)\n",
      "\u001b[0;31mValueError\u001b[0m: Input X contains infinity or a value too large for dtype('float64')."
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "from scipy.cluster.hierarchy import fcluster\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "year=2015\n",
    "def save_cluster_data(year):\n",
    "    df=pd.read_csv(f'cluster_{year}.csv')\n",
    "    #testdf = df[(df.MPG > 23) & (df.GP > 15) & (df.SEASON == '2022-23')].reset_index(drop=True)\n",
    "    \n",
    "    season=str(year-1)+'-'+str(year)[-2:]\n",
    "    print(season)\n",
    "    testdf = df[(df.MPG > 10) & (df.GP > 10) & (df.SEASON == season)].reset_index(drop=True)\n",
    "    print(testdf)\n",
    "    \n",
    "    features = [x for x in df.columns if (x != 'PLAYER_NAME') &  (x != 'POSITION') & (x != 'SEASON')]\n",
    "    \n",
    "    x = testdf.loc[:, features].values\n",
    "    y = testdf.loc[:,['PLAYER_NAME']].values\n",
    "    x = np.where(np.isinf(x), np.nan, x)  # Replace inf with NaN\n",
    "    x = np.nan_to_num(x)\n",
    "\n",
    "    x = StandardScaler().fit_transform(x) # standardize all values\n",
    "    \n",
    "    \n",
    "    pca = PCA(n_components=0.99)\n",
    "    principalComponents = pca.fit_transform(x)\n",
    "    \n",
    "    '''\n",
    "    plt.figure(figsize=(10,65))\n",
    "    plt.title('2024-25 NBA Hierarchical Clustering Dendrogram')\n",
    "    dend = shc.dendrogram(shc.linkage(x, method='ward'), labels=list(testdf.PLAYER_NAME), orientation='left')\n",
    "    \n",
    "    plt.yticks(fontsize=8)\n",
    "    plt.xlabel('Height')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    '''\n",
    "    # Save the plot in PNG format\n",
    "    #plt.savefig(\"nba_hierarchical_clustering_dendrogram.png\", format='png', dpi=300)\n",
    "    \n",
    "    # Optionally, display the plot as well\n",
    "    #plt.show()\n",
    "    \n",
    "    \n",
    "    # In[3]:\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # Assuming 'principalComponents' contains the PCA-transformed features and 'testdf' contains the player names\n",
    "    # Here 'x' is 'principalComponents' and 'y' is 'testdf['PLAYER_NAME']'\n",
    "    \n",
    "    # Convert the PCA feature vectors into a square-form distance matrix\n",
    "    x = principalComponents  # PCA-transformed feature matrix\n",
    "    y = testdf['PLAYER_NAME'].values  # Player names\n",
    "    z = testdf['PLAYER_ID'].values  # Player IDs\n",
    "    \n",
    "    # Apply hierarchical clustering \n",
    "    cluster_labels = fcluster(shc.linkage(x, method='ward'), t=25, criterion='distance')\n",
    "    \n",
    "    # Create a DataFrame mapping players to their cluster labels and IDs\n",
    "    player_cluster_mapping = pd.DataFrame({\n",
    "        'PLAYER_NAME': y, \n",
    "        'PLAYER_ID': z,\n",
    "        'CLUSTER': cluster_labels\n",
    "    })\n",
    "    \n",
    "    x = principalComponents  # PCA-transformed feature matrix\n",
    "    y = testdf['PLAYER_NAME'].values  # Player names\n",
    "    z = testdf['PLAYER_ID'].values  # Player IDs\n",
    "    \n",
    "    # Apply hierarchical clustering \n",
    "    cluster_labels = fcluster(shc.linkage(x, method='ward'), t=25, criterion='distance')\n",
    "    \n",
    "    # Create a DataFrame mapping players to their cluster labels and IDs\n",
    "    player_cluster_mapping = pd.DataFrame({\n",
    "       'PLAYER_NAME': y, \n",
    "       'PLAYER_ID': z,\n",
    "       'CLUSTER': cluster_labels\n",
    "    })\n",
    "    \n",
    "    # Prepare a DataFrame to hold each player and their 5 most similar players\n",
    "    similar_players_df = pd.DataFrame()\n",
    "    for idx, player in enumerate(y):\n",
    "       # Get the current player's cluster and ID\n",
    "       player_row = player_cluster_mapping[player_cluster_mapping['PLAYER_NAME'] == player]\n",
    "       cluster = player_row['CLUSTER'].iloc[0]\n",
    "       base_player_id = player_row['PLAYER_ID'].iloc[0]\n",
    "       \n",
    "       # Get indices of players in the same cluster\n",
    "       indices = player_cluster_mapping[player_cluster_mapping['CLUSTER'] == cluster].index\n",
    "       \n",
    "       # Calculate distances from the current player to others in the same cluster\n",
    "       distances = euclidean_distances(x[indices], [x[idx]]).flatten()\n",
    "       \n",
    "       # Get indices of the 5 closest players\n",
    "       closest_indices = np.argsort(distances)[1:6]  # Exclude the closest (itself)\n",
    "       \n",
    "       # Extract names, IDs, and distances of the 5 closest players\n",
    "       closest_players = player_cluster_mapping.iloc[indices[closest_indices]]\n",
    "       closest_distances = distances[closest_indices]\n",
    "       \n",
    "       # Create a new row with player name and ID, similar player names, IDs, and distances\n",
    "       new_row = pd.DataFrame([{\n",
    "           'PLAYER_NAME': player,\n",
    "           'PLAYER_ID': base_player_id,\n",
    "           'SIMILAR_1_NAME': closest_players['PLAYER_NAME'].iloc[0] if len(closest_players) > 0 else '',\n",
    "           'SIMILAR_1_ID': closest_players['PLAYER_ID'].iloc[0] if len(closest_players) > 0 else '',\n",
    "           'SIMILAR_1_DISTANCE': closest_distances[0] if len(closest_distances) > 0 else '',\n",
    "           'SIMILAR_2_NAME': closest_players['PLAYER_NAME'].iloc[1] if len(closest_players) > 1 else '',\n",
    "           'SIMILAR_2_ID': closest_players['PLAYER_ID'].iloc[1] if len(closest_players) > 1 else '',\n",
    "           'SIMILAR_2_DISTANCE': closest_distances[1] if len(closest_distances) > 1 else '',\n",
    "           'SIMILAR_3_NAME': closest_players['PLAYER_NAME'].iloc[2] if len(closest_players) > 2 else '',\n",
    "           'SIMILAR_3_ID': closest_players['PLAYER_ID'].iloc[2] if len(closest_players) > 2 else '',\n",
    "           'SIMILAR_3_DISTANCE': closest_distances[2] if len(closest_distances) > 2 else '',\n",
    "           'SIMILAR_4_NAME': closest_players['PLAYER_NAME'].iloc[3] if len(closest_players) > 3 else '',\n",
    "           'SIMILAR_4_ID': closest_players['PLAYER_ID'].iloc[3] if len(closest_players) > 3 else '',\n",
    "           'SIMILAR_4_DISTANCE': closest_distances[3] if len(closest_distances) > 3 else '',\n",
    "           'SIMILAR_5_NAME': closest_players['PLAYER_NAME'].iloc[4] if len(closest_players) > 4 else '',\n",
    "           'SIMILAR_5_ID': closest_players['PLAYER_ID'].iloc[4] if len(closest_players) > 4 else '',\n",
    "           'SIMILAR_5_DISTANCE': closest_distances[4] if len(closest_distances) > 4 else ''\n",
    "       }])\n",
    "       \n",
    "       similar_players_df = pd.concat([similar_players_df, new_row], ignore_index=True)\n",
    "    \n",
    "    # Export the DataFrame to a CSV file\n",
    "    similar_players_df.to_csv(f'{year}_similar_players.csv', index=False)\n",
    "    print(\"CSV file 'nba_similar_players.csv' has been created with each player and their 5 most similar players.\")\n",
    "    \n",
    "    \n",
    "    # In[4]:\n",
    "    \n",
    "    # Filter the DataFrame\n",
    "    testdf = df[(df.MPG > 10) & (df.GP > 10) & (df.SEASON == season)].reset_index(drop=True)\n",
    "    \n",
    "    # Select features for clustering\n",
    "    features = [x for x in df.columns if (x != 'PLAYER_NAME') & (x != 'POSITION') & (x != 'SEASON')]\n",
    "    \n",
    "    # Standardize the data\n",
    "    x = testdf.loc[:, features].values\n",
    "    x = np.where(np.isinf(x), np.nan, x)  # Replace inf with NaN\n",
    "    x = np.nan_to_num(x)\n",
    "    x = StandardScaler().fit_transform(x)\n",
    "    \n",
    "    # Perform PCA to reduce dimensionality\n",
    "    pca = PCA(n_components=2)  # Reduce to 2 components for visualization\n",
    "    principalComponents = pca.fit_transform(x)\n",
    "    testdf['PCA1'] = principalComponents[:, 0]\n",
    "    testdf['PCA2'] = principalComponents[:, 1]\n",
    "    \n",
    "    \n",
    "        \n",
    "        # Add PCA components to dataframe\n",
    "    for i in range(principalComponents.shape[1]):\n",
    "        testdf[f'PCA_{i+1}'] = principalComponents[:, i]\n",
    "    print(testdf.columns)\n",
    "    # Determine optimal clusters using elbow method and silhouette scores\n",
    "    inertia = []\n",
    "    silhouette_scores = []\n",
    "    k_range = range(2, 15)  # Range of cluster numbers to test\n",
    "    \n",
    "    for k in k_range:\n",
    "        kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "        kmeans.fit(principalComponents)\n",
    "        inertia.append(kmeans.inertia_)\n",
    "        silhouette_scores.append(silhouette_score(principalComponents, kmeans.labels_))\n",
    "    \n",
    "    # Find the elbow point using KneeLocator\n",
    "    knee_locator = KneeLocator(k_range, inertia, curve=\"convex\", direction=\"decreasing\")\n",
    "    optimal_clusters_elbow = knee_locator.knee\n",
    "    \n",
    "    # Find optimal clusters by silhouette score\n",
    "    optimal_clusters_silhouette = k_range[np.argmax(silhouette_scores)]\n",
    "    \n",
    "    # Plot the elbow method\n",
    "    '''\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.plot(k_range, inertia, 'bo-', label='Inertia')\n",
    "    plt.axvline(optimal_clusters_elbow, color='r', linestyle='--', label=f'Elbow Point: {optimal_clusters_elbow}')\n",
    "    plt.title(\"Elbow Method\")\n",
    "    plt.xlabel(\"Number of Clusters\")\n",
    "    plt.ylabel(\"Inertia\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"nba_pca_kmeans_elbow.png\", format='png', dpi=300)\n",
    "    #plt.show()\n",
    "    \n",
    "    # Plot silhouette scores\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.plot(k_range, silhouette_scores, 'go-', label='Silhouette Score')\n",
    "    plt.axvline(optimal_clusters_silhouette, color='r', linestyle='--', label=f'Best Silhouette: {optimal_clusters_silhouette}')\n",
    "    plt.title(\"Silhouette Scores\")\n",
    "    plt.xlabel(\"Number of Clusters\")\n",
    "    plt.ylabel(\"Silhouette Score\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"nba_pca_kmeans_silhouette.png\", format='png', dpi=300)\n",
    "    #plt.show()\n",
    "    '''\n",
    "    # Let user choose optimal clusters\n",
    "    print(f\"Optimal clusters (Elbow): {optimal_clusters_elbow}\")\n",
    "    print(f\"Optimal clusters (Silhouette): {optimal_clusters_silhouette}\")\n",
    "    try:\n",
    "        #user_clusters = int(input(\"Enter the number of clusters you prefer (press Enter to use the best silhouette score): \").strip() or optimal_clusters_silhouette)\n",
    "        user_clusters=8\n",
    "    except ValueError:\n",
    "        user_clusters = optimal_clusters_silhouette  # Fallback to silhouette-based optimal clusters\n",
    "    \n",
    "    print(f\"Using {user_clusters} clusters for final clustering.\")\n",
    "    \n",
    "    # Perform KMeans clustering with the chosen number of clusters\n",
    "    kmeans_final = KMeans(n_clusters=user_clusters, random_state=42)\n",
    "    kmeans_final.fit(principalComponents)\n",
    "    testdf['Cluster'] = kmeans_final.labels_\n",
    "    \n",
    "    testdf.to_csv(f'nba_analysis_{year}.csv',index=False)\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for year in range(2025,2026):\n",
    "    save_cluster_data(year)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
